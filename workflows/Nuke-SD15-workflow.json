{
  "last_node_id": 44,
  "last_link_id": 123,
  "nodes": [
    {
      "id": 3,
      "type": "KSampler",
      "pos": {
        "0": 802,
        "1": 172
      },
      "size": {
        "0": 238.9956817626953,
        "1": 262.90533447265625
      },
      "flags": {},
      "order": 7,
      "mode": 0,
      "inputs": [
        {
          "name": "model",
          "type": "MODEL",
          "link": 1
        },
        {
          "name": "positive",
          "type": "CONDITIONING",
          "link": 114
        },
        {
          "name": "negative",
          "type": "CONDITIONING",
          "link": 112
        },
        {
          "name": "latent_image",
          "type": "LATENT",
          "link": 2
        }
      ],
      "outputs": [
        {
          "name": "LATENT",
          "type": "LATENT",
          "links": [
            7
          ],
          "slot_index": 0
        }
      ],
      "properties": {
        "Node name for S&R": "KSampler"
      },
      "widgets_values": [
        939714898316,
        "fixed",
        40,
        7,
        "heun",
        "ddim_uniform",
        1
      ]
    },
    {
      "id": 4,
      "type": "CheckpointLoaderSimple",
      "pos": {
        "0": 116,
        "1": 274
      },
      "size": {
        "0": 336.9458312988281,
        "1": 98
      },
      "flags": {},
      "order": 4,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "MODEL",
          "type": "MODEL",
          "links": [
            1
          ],
          "slot_index": 0
        },
        {
          "name": "CLIP",
          "type": "CLIP",
          "links": [
            122,
            123
          ],
          "slot_index": 1
        },
        {
          "name": "VAE",
          "type": "VAE",
          "links": [
            8
          ],
          "slot_index": 2
        }
      ],
      "properties": {
        "Node name for S&R": "CheckpointLoaderSimple"
      },
      "widgets_values": [
        "v2-1_512-ema-pruned.safetensors"
      ]
    },
    {
      "id": 5,
      "type": "EmptyLatentImage",
      "pos": {
        "0": 805,
        "1": 20
      },
      "size": {
        "0": 210,
        "1": 106
      },
      "flags": {},
      "order": 0,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "LATENT",
          "type": "LATENT",
          "links": [
            2
          ],
          "slot_index": 0
        }
      ],
      "properties": {
        "Node name for S&R": "EmptyLatentImage"
      },
      "widgets_values": [
        1024,
        1024,
        4
      ]
    },
    {
      "id": 8,
      "type": "VAEDecode",
      "pos": {
        "0": 623,
        "1": 15
      },
      "size": {
        "0": 140,
        "1": 46
      },
      "flags": {},
      "order": 8,
      "mode": 0,
      "inputs": [
        {
          "name": "samples",
          "type": "LATENT",
          "link": 7
        },
        {
          "name": "vae",
          "type": "VAE",
          "link": 8
        }
      ],
      "outputs": [
        {
          "name": "IMAGE",
          "type": "IMAGE",
          "links": [
            9
          ],
          "slot_index": 0
        }
      ],
      "properties": {
        "Node name for S&R": "VAEDecode"
      },
      "widgets_values": []
    },
    {
      "id": 9,
      "type": "SaveImage",
      "pos": {
        "0": 1073,
        "1": 40
      },
      "size": {
        "0": 430.9452819824219,
        "1": 480.7397766113281
      },
      "flags": {},
      "order": 9,
      "mode": 0,
      "inputs": [
        {
          "name": "images",
          "type": "IMAGE",
          "link": 9
        }
      ],
      "outputs": [],
      "properties": {},
      "widgets_values": [
        "ComfyUI"
      ]
    },
    {
      "id": 37,
      "type": "CLIPLoader",
      "pos": {
        "0": 48,
        "1": 136
      },
      "size": {
        "0": 409.0445556640625,
        "1": 84.82955932617188
      },
      "flags": {},
      "order": 3,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "CLIP",
          "type": "CLIP",
          "links": [],
          "slot_index": 0
        }
      ],
      "properties": {
        "Node name for S&R": "CLIPLoader"
      },
      "widgets_values": [
        "ViT-L-14-GmP-SAE-TE-only.safetensors",
        "stable_diffusion"
      ]
    },
    {
      "id": 40,
      "type": "Note",
      "pos": {
        "0": 646,
        "1": 478
      },
      "size": {
        "0": 415.4067687988281,
        "1": 157.671875
      },
      "flags": {},
      "order": 1,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "properties": {},
      "widgets_values": [
        "How do I get an embedding? A special one, where the text prompt is made by CLIP itself, looking at an image and having a (crazy...) opinion about it?\n\nIt's easy, and it's pure CLIP - no other AI involved, no making this human-interpretable and meaningful (unlike with CLIP-interrogator, for example)!\n\nPull out your favorite image and run this:\nhttps://github.com/zer0int/CLIP-gradient-ascent-embeddings"
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 41,
      "type": "Note",
      "pos": {
        "0": 65,
        "1": 24
      },
      "size": {
        "0": 391.8866882324219,
        "1": 63.87260818481445
      },
      "flags": {},
      "order": 2,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "properties": {},
      "widgets_values": [
        "New model! https://huggingface.co/zer0int/CLIP-SAE-ViT-L-14"
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 42,
      "type": "CLIPTextEncodeNUKE",
      "pos": {
        "0": 476,
        "1": 113
      },
      "size": {
        "0": 311.7621765136719,
        "1": 244
      },
      "flags": {},
      "order": 6,
      "mode": 0,
      "inputs": [
        {
          "name": "clip",
          "type": "CLIP",
          "link": 123
        }
      ],
      "outputs": [
        {
          "name": "CONDITIONING",
          "type": "CONDITIONING",
          "links": [
            112
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "CLIPTextEncodeNUKE"
      },
      "widgets_values": [
        "kitty caturday",
        "False",
        "True",
        425533035839474,
        "fixed",
        "False",
        0,
        "embeds/customembedding.pt"
      ],
      "color": "#322",
      "bgcolor": "#533"
    },
    {
      "id": 43,
      "type": "CLIPTextEncodeNUKE",
      "pos": {
        "0": 221,
        "1": 415
      },
      "size": {
        "0": 402.8760986328125,
        "1": 279.5404357910156
      },
      "flags": {},
      "order": 5,
      "mode": 0,
      "inputs": [
        {
          "name": "clip",
          "type": "CLIP",
          "link": 122
        }
      ],
      "outputs": [
        {
          "name": "CONDITIONING",
          "type": "CONDITIONING",
          "links": [
            114
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "CLIPTextEncodeNUKE"
      },
      "widgets_values": [
        "randn: Guidance by a random distribution (surprise)!\nnuke: tensor full of zeros, no guidance.",
        "False",
        "True",
        425533035839474,
        "fixed",
        "False",
        0,
        "embeds/customembedding.pt"
      ],
      "color": "#232",
      "bgcolor": "#353"
    }
  ],
  "links": [
    [
      1,
      4,
      0,
      3,
      0,
      "MODEL"
    ],
    [
      2,
      5,
      0,
      3,
      3,
      "LATENT"
    ],
    [
      7,
      3,
      0,
      8,
      0,
      "LATENT"
    ],
    [
      8,
      4,
      2,
      8,
      1,
      "VAE"
    ],
    [
      9,
      8,
      0,
      9,
      0,
      "IMAGE"
    ],
    [
      112,
      42,
      0,
      3,
      2,
      "CONDITIONING"
    ],
    [
      114,
      43,
      0,
      3,
      1,
      "CONDITIONING"
    ],
    [
      122,
      4,
      1,
      43,
      0,
      "CLIP"
    ],
    [
      123,
      4,
      1,
      42,
      0,
      "CLIP"
    ]
  ],
  "groups": [],
  "config": {},
  "extra": {
    "ds": {
      "scale": 1.7715610000000022,
      "offset": [
        363.6679364029618,
        131.47297932711882
      ]
    }
  },
  "version": 0.4
}